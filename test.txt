

Wikipedia The Free Encyclopedia

    Donate
    Create account
    Log in

Banner logo

Wikimania 2025 Program Proposals are now open!
Click here to Apply Now
This application is open until Monday 31st March, 2025 end of day
[ Help with translations! ]
Contents
(Top)
Prehistory

Binary logic

    Gottfried Wilhelm Leibniz

Emergence of a discipline

    Charles Babbage and Ada Lovelace
    Early post-Analytical Engine designs
    Charles Sanders Peirce and electrical switching circuits
    Alan Turing and the Turing machine
    Kathleen Booth and the first assembly language
    Early computer hardware
    Shannon and information theory
    Wiener and cybernetics
    John von Neumann and the von Neumann architecture
    John McCarthy, Marvin Minsky and artificial intelligence

See also

References

        Sources
    Further reading
    External links

History of computer science

    Article
    Talk

    Read
    Edit
    View history

Tools

Appearance
Text

    Small
    Standard
    Large

Width

    Standard
    Wide

Color (beta)

    Automatic
    Light
    Dark

From Wikipedia, the free encyclopedia
History of computing
Hardware

    Hardware 1960s to present

Software

    Software Software configuration management Unix Free software and open-source software

Computer science

    Artificial intelligence Compiler construction Early computer science Operating systems Programming languages Prominent pioneers Software engineering

Modern concepts

    General-purpose CPUs Graphical user interface Internet Laptops Personal computers Video games World Wide Web Cloud Quantum

By country

    Bulgaria Eastern Bloc Poland Romania South America Soviet Union Yugoslavia

Timeline of computing

    before 1950 1950–1979 1980–1989 1990–1999 2000–2009 2010–2019 2020–present more timelines ...

Glossary of computer science

    Category

    vte

The history of computer science began long before the modern discipline of computer science, usually appearing in forms like mathematics or physics. Developments in previous centuries alluded to the discipline that we now know as computer science.[1] This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field, massive technological advancement across the Western world, and the basis of a massive worldwide trade and culture.[2]
Prehistory
John Napier (1550–1617), the inventor of logarithms

The earliest known tool for use in computation was the abacus, developed in the period between 2700 and 2300 BCE in Sumer.[3] The Sumerians' abacus consisted of a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.[4]: 11  Its original style of usage was by lines drawn in sand with pebbles. Abaci of a more modern design are still used as calculation tools today, such as the Chinese abacus.[5]

In the 5th century BC in ancient India, the grammarian Pāṇini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.[6]

The Antikythera mechanism is believed to be an early mechanical analog computer.[7] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.[7]

Mechanical analog computer devices appeared again a thousand years later in the medieval Islamic world. They were developed by Muslim astronomers, such as the mechanical geared astrolabe by Abū Rayhān al-Bīrūnī,[8] and the torquetum by Jabir ibn Aflah.[9] According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus.[10][11] Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Banū Mūsā brothers.[12]

Technological artifacts of similar complexity appeared in 14th century Europe, with mechanical astronomical clocks.[13]

When John Napier discovered logarithms for computational purposes in the early 17th century,[14] there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 Wilhelm Schickard designed a calculating machine as a commission for Johannes Kepler which he named the Calculating Clock, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624.[15] Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria.[16] Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.[17]

In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be Turing equivalent. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz.[18]

Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.
Binary logic
Gottfried Wilhelm Leibniz
Main article: Gottfried Wilhelm Leibniz
Gottfried Wilhelm Leibniz (1646–1716) developed logic in a binary number system and has been called the "founder of computer science".[19]

In 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system. Leibniz simplified the binary system and articulated logical properties such as conjunction, disjunction, negation, identity, inclusion, and the empty set.[20] He anticipated Lagrangian interpolation and algorithmic information theory. His calculus ratiocinator anticipated aspects of the universal Turing machine. In 1961, Norbert Wiener suggested that Leibniz should be considered the patron saint of cybernetics.[21] Wiener is quoted with "Indeed, the general idea of a computing machine is nothing but a mechanization of Leibniz's Calculus Ratiocinator."[22] But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled.[23]

By this time, the first mechanical devices driven by a binary pattern had been invented. The Industrial Revolution had driven forward the mechanization of many tasks, and this included weaving. Punched cards controlled Joseph Marie Jacquard's loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero. Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems and stored binary information.[23]
Emergence of a discipline
Charles Babbage (1791–1871), one of the pioneers of computing
Charles Babbage and Ada Lovelace
Main articles: Charles Babbage and Ada Lovelace

Charles Babbage is often regarded as one of the first pioneers of computing. Beginning in the 1810s, Babbage had a vision of mechanically computing numbers and tables. Putting this into reality, Babbage designed a calculator to compute numbers up to 8 decimal points long. Continuing with the success of this idea, Babbage worked to develop a machine that could compute numbers with up to 20 decimal places. By the 1830s, Babbage had devised a plan to develop a machine that could use punched cards to perform arithmetical operations. The machine would store numbers in memory units, and there would be a form of sequential control. This means that one operation would be carried out before another in such a way that the machine would produce an answer and not fail. This machine was to be known as the "Analytical Engine", which was the first true representation of what is the modern computer.[24]
Ada Lovelace (1815–1852) predicted the use of computers in symbolic manipulation

Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius. Lovelace began working with Charles Babbage as an assistant while Babbage was working on his "Analytical Engine", the first mechanical computer. [25] During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which could compute Bernoulli numbers,[26] although this is arguable as Charles was the first to design the difference engine and consequently its corresponding difference based algorithms, making him the first computer algorithm designer. Moreover, Lovelace's work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations but also manipulate symbols, mathematical or not.[27] While she was never able to see the results of her work, as the "Analytical Engine" was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.[28]
Early post-Analytical Engine designs
Leonardo Torres Quevedo (1852–1936) proposed a consistent manner to store floating-point numbers

Following Babbage, although at first unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.[29][30]

Two other inventors, Leonardo Torres Quevedo and Vannevar Bush, also did follow on research based on Babbage's work. In his Essays on Automatics (1914), Torres designed an analytical electromechanical machine that was controlled by a read-only program and introduced the idea of floating-point arithmetic.[31][32][33] In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, he presented in Paris the Electromechanical Arithmometer, which consisted of an arithmetic unit connected to a (possibly remote) typewriter, on which commands could be typed and the results printed automatically.[34] Bush's paper Instrumental Analysis (1936) discussed using existing IBM punch card machines to implement Babbage's design. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.[35]
Charles Sanders Peirce and electrical switching circuits
Charles Sanders Peirce (1839–1914) described how logical operations could be carried out by electrical switching circuits

In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits.[36] During 1880–81 he showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but this work on it was unpublished until 1933.[37] The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called Peirce's arrow.[38] Consequently, these gates are sometimes called universal logic gates.[39]